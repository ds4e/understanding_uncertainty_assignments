{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94165f3d",
   "metadata": {},
   "source": [
    "# Assignment 7\n",
    "### Do any three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b03aa8",
   "metadata": {},
   "source": [
    "## 1. \n",
    "\n",
    "- What is the expected value of a single die roll? \n",
    "- What is the expected value of rolling two dice and adding the results together?\n",
    "- What is the expected winnings of any gamble in European roulette?\n",
    "\n",
    "- Imagine you roll a die, and you record the value you get. But, if you roll a six, you roll again, and add that value. What is the expected value?\n",
    "- Imagine that the process described in the last question continues until you fail to roll a six. What is the expected value of the process? (This can be tricky, you can simulate it to get an answer if you prefer. Hint: The answer is 4.2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364a41c",
   "metadata": {},
   "source": [
    "## 2. \n",
    "- Compute the expected value for a uniform random variable.\n",
    "- Show that $\\mathbb{E}[a+bX] = a + b\\mathbb{E}[X]$\n",
    "- Show, by example, that $v(\\mathbb{E}[X]) \\neq \\mathbb{E}[v(X)]$, if $v(x) \\neq a+bx$. For example, try $v(y) = y^2$ or $v(y)=\\sqrt{y}$ with a Bernoulli or uniform or normally distributed random variable. This can be an important thing to remember: The expectation of a transformed random variable is not the transformation of the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56564ad3",
   "metadata": {},
   "source": [
    "## 3. \n",
    "- Compute the variance for a uniform random variable.\n",
    "- Show that \n",
    "$$\n",
    "\\mathbb{V}[X] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n",
    "$$\n",
    "$$\n",
    "\\mathbb{V}[a+bX] = b^2 \\mathbb{V}[X]\n",
    "$$\n",
    "- Show that if $X$ is a normally distributed random variable, then $a + bX$ is distributed normally with mean $a+ b \\mathbb{E}[X]$ and variance $b^2 \\sigma_X^2$ \n",
    "\n",
    "These properties get used all the time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6744a1",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "- The **covariance** of $X$ and $Y$ is\n",
    "$$\n",
    "\\text{cov}(X,Y) = \\int_{y} \\int_{x} (x-\\mathbb{E}[X])(y-\\mathbb{E}[Y])f_{XY}(x,y) dxdy = \\mathbb{E}_{XY}[ (x-\\mu_X)(y-\\mu_Y)]\n",
    "$$\n",
    "- Show that if $f_{XY}(x,y)=f_X(x)f_Y(y)$, then $\\text{cov}(X,Y)=0$\n",
    "- Provide an example (computation/simulation is fine) where $\\text{cov}(X,Y)\\approx 0$ but $f_{XY}(x,y)\\neq 0$\n",
    "- The covariance doesn't characterize joint random variables except in a few special cases: The covariance only captures the **linear** association between the two variables, not nonlinear associations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6734bc1",
   "metadata": {},
   "source": [
    "## 5. \n",
    "\n",
    "Suppose $X$ has an expectation $\\mathbb{E}[X]<\\infty$ and variance $\\mathbb{V}[X]<\\infty$; this isn't always true, but is *usually* true\n",
    "- Consider making a new variable, $\\varepsilon = X - \\mathbb{E}[X]$\n",
    "- What's the expectation of $\\varepsilon$?\n",
    "- What's the variance of $\\varepsilon$?\n",
    "- So we can write any random variable in the form $X = \\mathbb{E}[X] + \\varepsilon, $ where $\\mathbb{E}[\\varepsilon]=0$ and $\\mathbb{V}[\\varepsilon] = \\sigma_X^2$\n",
    "- If that's true, show that we can also write any random variable in the form $X = \\mathbb{E}[X] + \\sigma_X \\varepsilon$, where $\\mathbb{E}[\\varepsilon]=0$ and $\\mathbb{V}[\\varepsilon]=1$\n",
    "- Now replace $\\mathbb{E}[X]$ with $x\\beta$, and the stage is set for regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd39cd",
   "metadata": {},
   "source": [
    "## 6.\n",
    "- Use the Taylor series expansions \n",
    "$$\n",
    "F(x+h) = F(x) + hf(x) + \\frac{h^2}{2}f'(x) + O(h^3)\n",
    "$$ \n",
    "and \n",
    "$$\n",
    "F(x-h) = F(x) - h f(x) + \\frac{h^2}{2} f'(x)+ O(h^3)\n",
    "$$\n",
    "to show that\n",
    "$$\n",
    "\\mathbb{E}[\\hat{f}_{X,h}(x)] = \\frac{F(x+h)-F(x-h)}{2h} = f(x) + O(h^2),\n",
    "$$\n",
    "so the **bias** of the KDE is $O(h^2)$, unlike the ECDF, for which $\\mathbb{E}[\\hat{F}(x)] = F(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cc8aa",
   "metadata": {},
   "source": [
    "## 7.\n",
    "- Suppose $X$ and $Y$ are distributed bivariate normal. Show that if $\\rho=0$, then $X$ and $Y$ are independent.\n",
    "- For the multivariate normal, show that if $\\Sigma$ is a diagonal matrix, then $X_1, X_2, ..., X_n$ are independent.\n",
    "- For the multivariate normal, show that if $\\Sigma$ is a diagonal matrix and all the $\\sigma_i^2$ and all the $\\mu_i$ are equal, then $X_1, X_2, ..., X_n$ are independently distributed random variables with distribution $N(\\mu, \\sigma^2)$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
